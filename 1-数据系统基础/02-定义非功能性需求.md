# 简介

如果你正在构建一个应用程序，你将会被一系列需求所驱动。在你的需求列表中，最重要的可能是应用程序必须提供的功能：需要哪些界面和按钮，以及每个操作应该做什么，以实现软件的目的。这些是你的 ***功能性需求***。

此外，你可能还有一些 ***非功能性需求***：例如，应用程序应该快速、可靠、安全、合规，并且易于维护。这些需求可能没有明确写下来，但它们与应用程序的功能同样重要。

我们将考虑以下一些非功能性需求

- 如何定义和衡量系统的 **性能**
- 服务 **可靠** 意味着什么——即出现问题也能继续正确工作
- 通过在系统负载增长时添加计算能力的有效方法，使系统具有 **可伸缩性**
- 使系统长期更 **易于维护**



# 案例研究：社交网络首页时间线

你被赋予了实现一个类似 X（前身为 Twitter）风格的社交网络的任务，用户可以发布消息并关注其他用户。

假设用户每天发布 5 亿条帖子，或平均每秒 5,700 条帖子；偶尔速率可能飙升至每秒 150,000 条帖子。我们还假设平均每个用户关注 200 人并有 200 个粉丝；实际上大多数人只有少数粉丝，而少数名人如巴拉克・奥巴马有超过 1 亿粉丝。

## 表示用户、帖子与关注关系

假设我们将所有数据保存在关系数据库中，如 图 2-1 所示。我们有一个用户表、一个帖子表和一个关注关系表。

![img](02-定义非功能性需求.assets/ddia_0201.png)

<center style="color:#000;text-decoration:underline">图 2-1</center>



假设我们的社交网络必须支持的主要读取操作是 *首页时间线*，它显示你关注的人最近发布的帖子。我们可以编写以下 SQL 查询来获取特定用户的首页时间线：

```sql
SELECT posts.*, users.* FROM posts # 关注人的推文和个人信息
    JOIN follows ON posts.sender_id = follows.followee_id
    JOIN users ON posts.sender_id = users.id
    WHERE follows.follower_id = current_user # 查询当前用户关注了哪些人
    ORDER BY posts.timestamp DESC
    LIMIT 1000
```



假设在某人发布帖子后，我们希望他们的粉丝能够在 5 秒内看到它。一种方法是让用户的客户端每 5 秒重复上述查询（这称为 *轮询*）。如果我们假设有 1000 万用户同时在线登录，这意味着每秒运行 200 万次查询。即使增加轮询间隔，这也是很大的负载。

此外上述查询相当昂贵：如果你关注 200 人，它需要获取这 200 人中每个人的最近帖子列表，并合并这些列表。每秒 200 万次时间线查询意味着数据库需要每秒查找某个发送者的最近帖子 4 亿次。一些用户关注数万个账户；对他们来说，这个查询执行起来非常昂贵，而且很难快速完成。

## 时间线的物化与更新

首先，与其轮询，不如服务器主动向当前在线的任何粉丝推送新帖子。其次，我们应该预先计算上述查询的结果，以便可以从缓存中提供用户的首页时间线请求。



我们为每个用户存储一个包含其首页时间线的数据结构。每次用户发布帖子时，我们查找他们的所有粉丝，并将该帖子插入到每个粉丝的首页时间线中——就像向邮箱投递消息一样。

这种方法的缺点是，现在每次用户发布帖子时我们需要做更多的工作，因为**首页时间线是需要更新的派生数据**。该过程如 图 2-2 所示。当一个初始请求导致几个下游请求被执行时，我们使用术语 **扇出** 来描述请求数量增加的因子。

![img](02-定义非功能性需求.assets/ddia_0202.png)

<center style="color:#000;text-decoration:underline">图 2-2</center>

以每秒 5,700 条帖子的速率，如果平均帖子到达 200 个粉丝（即扇出因子为 200），我们将需要每秒执行超过 100 万次首页时间线写入。与我们本来需要的每秒 4 亿次每个发送者的帖子查找相比，这仍然是一个显著的节省。

如果由于某些特殊事件导致帖子速率激增，我们不必立即进行时间线交付——我们可以将它们**排队**，帖子在粉丝的时间线中显示会暂时花费更长时间。即使在这种负载峰值期间，时间线仍然可以快速加载，因为我们只是从缓存中提供它们。



这种预先计算和更新查询结果的过程称为 **物化**，时间线缓存是 **物化视图** 的一个例子。物化视图加速了读取，但作为交换，我们必须在写入时做更多的工作。

对于大多数用户来说，写入成本是适中的，但社交网络还必须考虑一些极端情况：

- 如果用户关注非常多的账户，并且这些账户发布很多内容，该用户的物化时间线将有很高的写入率。然而用户实际上不太可能阅读其时间线中的所有帖子，因此可以简单地丢弃其时间线的一些写入，只向用户显示他们关注的账户的**帖子样本**。
- 当拥有大量粉丝的名人账户发布帖子时，我们必须做大量工作将该帖子插入到他们数百万粉丝的每个首页时间线中。在这种情况下，丢弃一些写入是不可接受的。解决这个问题的一种方法是**将名人帖子与其他人的帖子分开处理**：我们可以通过将名人帖子单独存储并在读取时与物化时间线合并，来节省将它们添加到数百万时间线的工作。尽管有这些优化，处理社交网络上的名人仍然需要大量基础设施。



# 描述性能

大多数关于软件性能的讨论都考虑两种主要的度量类型：

- **响应时间**：从用户发出请求到收到请求应答所经过的时间。测量单位是秒（或毫秒，或微秒）。
- **吞吐量**：系统正在处理的每秒请求数，或每秒数据量。对于给定的硬件资源分配，存在可以处理的 *最大吞吐量*。测量单位是"每秒某物"。

在社交网络案例研究中，“每秒帖子数"和"每秒时间线写入数"是吞吐量指标，而"加载首页时间线所需的时间"或"帖子传递给粉丝的时间"是响应时间指标。

响应时间通常是用户最关心的，而吞吐量决定了所需的计算资源（例如需要多少服务器），因此决定了服务特定工作负载的成本。如果吞吐量可能会增长超出当前硬件可以处理的范围，则需要扩展容量；如果系统的最大吞吐量可以通过添加计算资源显著增加，则称系统为 **可伸缩的**。



吞吐量和响应时间之间通常存在联系。当请求吞吐量较低时，服务具有较低的响应时间，但随着负载增加，响应时间也会增加。

这是因为 **排队**：当请求到达高负载系统时，CPU 很可能已经在处理先前的请求，因此传入请求需要等待先前请求完成。随着吞吐量接近硬件可以处理的最大值，排队延迟急剧增加。

如果系统接近过载，吞吐量被推到极限附近，它有时会进入恶性循环，变得效率更低，从而更加过载。例如，如果有很长的请求队列等待处理，响应时间可能会增加到客户端超时并重新发送请求的程度。这导致请求率进一步增加，使问题变得更糟——**重试风暴**。

即使负载再次降低，这样的系统也可能保持过载状态，直到重新启动或以其他方式重置。这种现象称为 **亚稳态故障（Metastable Failure）**，它可能导致生产系统的严重中断。



为了避免重试使服务过载，你可以在客户端增加并随机化连续重试之间的时间（**指数退避**），并暂时停止向最近返回错误或超时的服务发送请求（使用 **熔断器** 或 **令牌桶** 算法）。服务器还可以检测何时接近过载并开始主动拒绝请求（**负载卸除**），并发送响应要求客户端减速（**背压**）。**排队和负载均衡算法**的选择也可能产生影响 。

## 延迟与响应时间

“延迟"和"响应时间"有时可互换使用，但在本书中我们将以特定方式使用这些术语（如 图 2-4 所示）：

- **响应时间** 是客户端看到的；它包括系统中任何地方产生的所有延迟。
- **服务时间** 是服务主动处理用户请求的持续时间。
- **排队延迟** 可能发生在流程中的几个点：例如，在收到请求后，它可能需要等待直到 CPU 可用才能被处理；如果同一台机器上的其他任务通过出站网络接口发送大量数据，响应数据包可能需要在发送之前进行缓冲。
- **延迟** 是一个涵盖请求未被主动处理时间的总称，即在此期间它是 *潜在的*。特别是，*网络延迟* 指的是请求和响应在网络中传输所花费的时间。

![img](02-定义非功能性需求.assets/ddia_0204.png)

<center style="color:#000;text-decoration:underline">图 2-4</center>

响应时间可能会因请求而异，即使你一遍又一遍地发出相同的请求。许多因素可能会增加随机延迟：例如，上下文切换到后台进程、网络数据包丢失和 TCP 重传、垃圾回收暂停、强制从磁盘读取的缺页错误、服务器机架中的机械振动，或许多其他原因。

排队延迟通常占响应时间变化的很大一部分。由于服务器只能并行处理少量事务（例如，受其 CPU 核心数的限制），只需要少量慢请求就可以阻塞后续请求的处理——这种效应称为 **队头阻塞**。排队延迟不是服务时间的一部分，因此在客户端测量响应时间很重要。

### 平均值、中位数与百分位数

因为响应时间因请求而异，我们需要将其视为值的 **分布**，而不是单个数字。在 图 2-5 中，每个灰色条表示对服务的请求，其高度显示该请求花费的时间。大多数请求相当快，但偶尔会有 *异常值* 需要更长时间。网络延迟的变化也称为 **抖动**。

![img](02-定义非功能性需求.assets/ddia_0205.png)

<center style="color:#000;text-decoration:underline">图 2-5</center>

平均响应时间对于估计吞吐量限制很有用。然而，如果你想知道你的"典型"响应时间，平均值不是一个很好的指标，因为它不能告诉你有多少用户实际经历了哪种延迟。

通常使用 **百分位数** 更好。如果你将响应时间列表从最快到最慢排序，那么 *中位数* 就在中间。中位数也称为 *第 50 百分位*，有时缩写为 *p50*。

为了弄清异常值有多糟糕，你可以查看更高的百分位数：*第 95*、*99* 和 *99.9* 百分位数很常见（缩写为 *p95*、*p99* 和 *p999*）。它们是 95%、99% 或 99.9% 的请求比该特定阈值快的响应时间阈值。

响应时间的高百分位数，也称为 **尾部延迟**，很重要，因为它们直接影响用户的服务体验。例如，亚马逊在描述内部服务的响应时间要求时使用第 99.9 百分位，即使它只影响 1,000 个请求中的 1 个。这是因为**请求最慢的客户通常是那些账户上数据最多的客户**，因为他们进行了许多购买——也就是说，他们是最有价值的客户。

另一方面，优化第 99.99 百分位（10,000 个请求中最慢的 1 个）被认为太昂贵。在非常高的百分位数上减少响应时间很困难，因为它们很容易受到你无法控制的随机事件的影响，而且收益递减。

#### 计算百分位数

最简单的实现是在时间窗口内保留所有请求的响应时间列表，并每分钟对该列表进行排序。如果这对你来说效率太低，有一些算法可以以最小的 CPU 和内存成本计算百分位数的良好近似值。开源百分位数估计库包括 HdrHistogram、t-digest、OpenHistogram 和 DDSketch。

### 响应时间指标的应用

高百分位数在一次请求中要多次调用其他服务的后端服务中尤其重要。即使你并行进行调用，最终用户请求仍然需要等待最慢的并行调用完成。只需要一个慢调用就可以使整个最终用户请求变慢。获得慢调用的机会会增加，因此更高比例的最终用户请求最终会变慢（这种效应称为 **尾部延迟放大**）。

百分位数通常用于 **服务级别目标（SLO）和 服务级别协议（SLA）**，作为定义服务预期性能和可用性的方式。例如，SLO 可能设定服务的中位响应时间小于 200 毫秒且第 99 百分位低于 1 秒的目标，以及至少 99.9% 的有效请求产生非错误响应的目标。SLA 是一份合同，规定如果不满足 SLO 会发生什么（例如，客户可能有权获得退款）。

## 可靠性与容错
